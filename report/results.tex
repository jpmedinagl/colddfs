\section{Results}
\label{s:results}

\subsection{Allocation Policy Effects}
\label{s:policy_effects}

The initial purpose of the simulator was to benchmark and investigate different
block allocation strategies on the simulator. Attempting to see the effects
of different workloads on the distributed file system.

However, due to time constraints, the simulator is entirely sequential. Meaning
requests are sent one at a time from the metadata node to the data nodes, and
requests are waited on by the metadata node until there's a response.

This means that different allocation policies did not seem to have a large
effect on the performance of the simulator.

\subsubsection{File Aware Tuning}
\label{s:fileawaretuning}
\

The file aware allocation policy works by switching between a random policy
versus least loaded when the number of allocated blocks at the same time are 
below a small file threshold.

These tests were ran on the implementation of preallocate. Meaning the file 
would be allocated all at once during creation, and thus we know whether it
will be a small file or large file depending on its size. On later implementations
like postallocate, this allocation policy will shift in meaning from small file
vs large file, to small allocation group vs large allocation group. This is 
because with lazy allocation, we might not allocate the entire file at once. 
This has the drawback that we could create a large file by doing small file 
allocations at a time. This will make it look like a small allocation group
every time and use random, and we would have a large file allocated distributed
randomly though it is a large file. For the later implementations, a more clever
file aware allocation policy should be created.

The small file threshold can be tuned since random allocation is much faster than
least loaded. In the following experiment, a block size of 4096KB was used

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/fileawaretune/avg_write_latency_ms4k.png}}
\caption{}
\label{f:}
\end{figure}

\subsection{File Distribution Impact}
\label{s:file_dist_impact}

\subsection{Data Node Scaling}
\label{s:datanode_scaling}

The number of data nodes in the application was expected to have an effect on
the performance of the application. However, due to the sequential nature of the
simulator, the data node number did not affect the performance of the file system.

Since the simulator was sequential, the metadata node sending a request to a 
datanode looks the same regardless of the data node. This is because nodes do
not have outstanding requests, each node handles one request at a time and 
responds to the metadata node. And thus sending to any data node looks the same.

Thus increasing the number of data nodes, wouldn't affect the application,
having a single data node handle all requests vs having 16 data nodes handle
the requests look the same in the simulator.

A future solution for this would be metadata node parallelism and datanode parallelism.
Though similar these two solutions are different. Metadata node parallelism means
that the metadata node handles each datanode in parallel, and thus can send requests
to two datanodes in parallel. However the metadata node cannot send two requests
to the same datanode since the datanodes are not parallel. With datanode parallelism,
this means that the datanodes can now handle multiple outstanding requests, meaning
the metadata node can send two requests to the same datanode and the datanode will
handle the requests in parallel.

Metadata node parallelism would not be difficult to implement, the metadata node
could create threads for each of the data nodes and each thread would wait until
the involved thread responds. Each thread would write (if its a read request)
into a different area of the buffer and thus would have no issues with corruption.

Datanode parallelism would be slightly more difficult to implement. This involves
locking mechanisms as now the datanode has to protect file blocks from being
read and written to at the same time.

With these new ideas, they could give an insight to data node scaling as the
metadata node can send more than one request to each data node. Increasing
the number of datanodes would likely increase performance, but at the cost
of metadata overhead and complexity.

\subsection{Optimal Block Size}
\label{s:extend_based_blocks}

The block size is a pretty important parameter in distributed file systems.
Distributed file systems do not have to adhere to any strict block sizes since
file contents are stored in data nodes own file system as files. This means
that blocks can be as small or large as required.

Small block sizes (4KB, 8KB) although used in file systems, are not ideal for
distributed file systems since distributed file systems communicate through the
network. This means that sending a larger request can be more beneficial as we're
already sending a request.

Larger block sizes can be beneficial, but too large of block sizes and the 
bottleneck of the simulator will be sending the requests.



\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/blocks/write_throughput_mbps.pdf}}
\caption{}
\label{f:}
\end{figure}

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/blocks/avg_write_latency_ms.pdf}}
\caption{}
\label{f:}
\end{figure}

Figure \ref{} outlines the experiment of measuring the average write latency versus
the block size. We can observe a minimum around the 32KB-64KB block size. This
means that for this workload, having block sizes wich are small/mid sized
allows us to minimize the write latency. This is because really small block sizes
increase the number of requests to the simulator, and this becomes the bottleneck.
Really large block sizes minimize the number of requests but lengthen the time
to process requests which becomes the bottleneck.  

\subsection{Implementation Benchmarks}
\label{s:implementation_bench}

In the previous section, it was outlined that various different implementations
were considered. Specifically, pre-allocate - allocating on file creation, 
post-allocate - allocating on write, batch-allocate - batch allocate blocks
sent to same node.

We can do a simple benchmark over these implementations and observe different
tradeoffs between them.

The experiment was done as follows.
% \experiment{
% 4 datanodes
% 16 MB file created, written to, and then read
% total size, 16 MB * 4 (so each node can fully hold file
% }

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/latency_impl.png}}
\caption{}
\label{f:}
\end{figure}

Figure \ref{} shows the results of the experiment. Each group of bars represents
a different implementation. Each bar in the group represents the different quantities
measured, create latency, write latency, and read latency, as well as the total latency
shown in red.

We can observe that the total latency between the preallocate and postallocate 
implementations are the same. This is because in this case, there are no empty 
blocks in the file, all blocks are allocated during the write, and thus both 
implementations will take the same amount of time. However we can see the 
difference between these two implementations by looking at the create and write 
latencies. Since pre-allocate allocates on create, the create latency is much larger
than post-allocate as post-allocate only modifies metadata. However we can see that
the write latency for the pre-allocate is much lower than post-allocate. This is
because the pre-allocate implementation does more work before the write, and thus
has faster writes.

We can thus see a tradeoff between these two implementations. If speed takes priority
in a write-heavy workload, then doing pre-allocation would be a good idea because
creating the file can reduce the write latency. However as mentioned previously,
pre-allocate potentially takes up more space as the entire file is allocated when
the full file may not be written to. Thus if storage takes priority, post-allocation
would be a better implementation as this can save us from allocating empty blocks
in a file (though in a typical file system this may not be common).

We can now compare these implementations to the batch allocation. As mentioned
in an earlier section, batch allocation works by batching the blocks sent to one
data node together, regardless of the file order. This saves on number of requests
sent by the metadata node.

We can see that the create latency is as fast as post-allocate because it only involves
metadata modification. However we can see a much 

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/block/read.png}}
\caption{}
\label{f:}
\end{figure}

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/block/write.png}}
\caption{}
\label{f:}
\end{figure}



% \begin{figure}[H]
% \centerline{\includegraphics[width=\columnwidth]{}}
% \caption{}
% \label{f:}
% \end{figure}
