\section{Results}
\label{s:results}

\subsection{Allocation Policy Effects}
\label{s:policy_effects}

The initial purpose of the simulator was to benchmark and investigate different
block allocation strategies and attempting to see the effects
of different workloads on the distributed file system.

However, the current design of the simulator is entirely sequential. Thus
requests are sent one at a time from the metadata node to the data nodes, and
requests are waited on by the metadata node until there's a response.
This resulted in different allocation policies performing similarly and not
having a large effect on the performance of the simulator.

\subsection{File Aware Tuning}
\label{s:fileawaretuning}

The file aware allocation policy works by switching between a random policy
versus least loaded policy when the number of allocated blocks at the same time are 
below a small file threshold.

The following test was ran on the implementation of preallocate. The file 
would be allocated all at once during creation, and thus the simulator could
differentiate between small files or large files depending on their creation
size. On later implementations
like postallocate, this allocation policy will shift in meaning from small file
vs large file, to small allocation group vs large allocation group. This is 
because with lazy allocation, we might not allocate the entire file at once. 
This has the drawback that we could create a large file by doing small file 
allocations at a time. This will make it look like a small allocation group
every time and use random, and we would have a large file allocated distributed
randomly though it is a large file. For the later implementations, a more clever
file aware allocation policy should be created.

The small file threshold can be tuned since random allocation is much faster than
least loaded. The reason being because least loaded requires looping through all
of the data nodes and keeping track of the least loaded node.

% EXPERIMENT
% BLOCK_SIZE = 4096KB
% DATANODES = 8
% CAPACITY = 16MB
% DISTRIBUTION = UNIFORMLY LARGE FILES

\begin{table}[H]
    \centering
    \caption{File Aware Tuning Experiment}
    \label{t:fileaware_tune_table}
    \begin{tabular}{l|l}
        Capacity & 16 MB \\
        Datanodes & 8 \\
        Block Size & 4096 KB \\
        Distribution & UNIFORM LARGE \\
    \end{tabular}
\end{table}

\begin{figure}[H]
    \centerline{\includegraphics[width=\columnwidth]{plots/fileawaretune/avg_write_latency_ms4k.png}}
    \caption{Average Write Latency vs Threshold Size for File Aware Policy. Tuning the
    small file threshold.}
    \label{f:fileaware_tune}
\end{figure}

\autoref{t:fileaware_tune_table} outlines the parameters of the experiment.
The purpose of the experiment was to observe the effect of the small file threshold
parameter with respect to the file aware tuning policy. The average write latency
was measured for each threshold.

The results of the experiment in Figure \ref{f:fileaware_tune} show that the
small file threshold minimizes the average write latency at around 4 blocks.
Having the small file threshold have less blocks (1-2 blocks) makes it so that
virtually every file uses the least loaded policy which is slow. However making
the small file threshold have more blocks (16+ blocks) makes it so that large
files use random policy, that although fast, can start to get slower. The reason
for this being that for very large files, least loaded picks the least loaded node
and then sends all the requests to this node. For a large file using the random
policy means that for every block, we have the overhead of randomly picking a node.

It's important to note that this specific experiment does not show that the small
file threshold minimizes latency at 4 blocks for all workloads. The effects we
see are due to the block size we are using, the number of data nodes, and the
file distribution. However this experiment shows that the simulator can show
that optimizations can be done for different workloads.

% \subsection{File Distribution Impact}
% \label{s:file_dist_impact}

\subsection{Data Node Scaling}
\label{s:datanode_scaling}

The number of data nodes in the application was expected to have an effect on
the performance of the application. However, due to the sequential nature of the
simulator, the data node number did not affect the performance of the file system.

Since the simulator was sequential, the metadata node sending a request to a 
data node looks the same regardless of the data node. This is because nodes do
not have outstanding requests, each node handles one request at a time and 
responds to the metadata node. And thus sending to any data node looks the same.

Thus increasing the number of data nodes, wouldn't affect the application,
having a single data node handle all requests vs having 16 data nodes handle
the requests look the same in the simulator.

A future solution for this would be metadata node parallelism and datanode parallelism.
Though similar these two solutions are different. Metadata node parallelism means
that the metadata node handles each datanode in parallel, and thus can send requests
to two datanodes in parallel. However the metadata node cannot send two requests
to the same datanode since the datanodes are not parallel. With datanode parallelism,
this means that the datanodes can now handle multiple outstanding requests, meaning
the metadata node can send two requests to the same datanode and the datanode will
handle the requests in parallel.

Metadata node parallelism would not be difficult to implement, the metadata node
could create threads for each of the data nodes and each thread would wait until
the involved thread responds. Each thread would write (if its a read request)
into a different area of the buffer and thus would have no issues with corruption.

Datanode parallelism would be slightly more difficult to implement. This involves
locking mechanisms as now the datanode has to protect file blocks from being
read and written to at the same time.

With these new ideas, they could give an insight to data node scaling as the
metadata node can send more than one request to each data node. Increasing
the number of datanodes would likely increase performance, but at the cost
of metadata overhead and complexity.

\subsection{Optimal Block Size}
\label{s:extend_based_blocks}

The block size is a pretty important parameter in distributed file systems.
Distributed file systems do not have to adhere to any block sizes since
file contents are stored in data nodes own file system as files. This means
that blocks can be as small or large as required.

Small block sizes (4KB, 8KB) although used in file systems, are not ideal for
distributed file systems since distributed file systems communicate through the
network. This means that sending a larger request can be more beneficial as we're
already sending a request.

Larger block sizes can be beneficial, but too large of block sizes and the 
bottleneck of the simulator will be sending the requests.

\begin{table}[H]
    \centering
    \caption{Optimal Block Size Experiment}
    \label{t:optimal_block_size}
    \begin{tabular}{l|l}
        Capacity & 16 MB \\
        Datanodes & 8 \\
        Operations & 500 \\
        Distribution & WEB \\
    \end{tabular}
\end{table}

\begin{figure}[h]
\centerline{\includegraphics[width=\columnwidth]{plots/blocks/write_throughput_mbps.pdf}}
\caption{Write throughput vs Block Size for the experiment outlined in 
\autoref{t:optimal_block_size}.}
\label{f:optimal_block_throughput}
\end{figure}

\begin{figure}[h]
\centerline{\includegraphics[width=\columnwidth]{plots/blocks/avg_write_latency_ms.pdf}}
\caption{Average Write Latency vs Block Size for the experiment outlined in
\autoref{t:optimal_block_size}.}
\label{f:optimal_block_latency}
\end{figure}

\autoref{t:optimal_block_size} outlines the experiment for measuring the performance
of the simulator against different block sizes.

Figure \ref{f:optimal_block_throughput} shows the write throughput versus
the block size for the experiment. We can observe that the throughput increases
fast at first and then steadily evens out.

Figure \ref{f:optimal_block_latency} shows the average write latency versus
the block size for the experiment.
We can observe a minimum around the 32KB-64KB block size. This
means that for this workload, having block sizes which are small/mid sized
allows us to minimize the write latency. 
The reason for this being that too small block sizes increase the number of
requests from the metadata node, and this becomes the bottleneck as there are too
many requests. However, too large block sizes minimize the number of requests
but they lengthen the time to process the requests, the writing becomes the
bottleneck. 

\subsection{Implementation Benchmarks}
\label{s:implementation_bench}

In the previous section, it was outlined that various different implementations
for the simulator were considered. Specifically, pre-allocate - allocating on 
file creation, post-allocate - allocating on write, batch-allocate - batch 
allocate blocks sent to same node.
We can do a simple benchmark over these implementations and observe different
tradeoffs between them.

% \experiment{
% 3 datanodes
% 100 block file created, written to, and then read
% total size file size * 3
% roundrobin
% block size 4096
% }

\begin{table}[H]
    \centering
    \caption{Allocate Implementation Benchmark}
    \label{t:allocate_impl_bench}
    \begin{tabular}{l|l}
        Capacity & 100 blocks * 3 \\
        Datanodes & 3 \\
        Block Size & 4096 KB \\
        Policy & Round Robin \\
    \end{tabular}
\end{table}

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/latency_impl.png}}
\caption{Latency per Operation versus allocation implementation for the experiment
in \autoref{t:allocate_impl_bench}.}
\label{f:batch_latency}
\end{figure}

Figure \ref{f:batch_latency} shows the results of the experiment. Each group of bars represents
a different implementation. Each bar in the group represents the different quantities
measured, the create latency, write latency, and read latency, as well as the total latency
shown in red.

We can observe that the total latency between the preallocate and postallocate 
implementations are the same. This is because in this case, there are no empty 
blocks in the file, all blocks are allocated during the write, and thus both 
implementations will take the same amount of total time. However we can see the 
difference between these two implementations by looking at the create and write 
latencies. Since pre-allocate allocates on create, the create latency is much larger
than post-allocate as post-allocate only modifies metadata. However we can see that
the write latency for the pre-allocate is much lower than post-allocate. This is
because the pre-allocate implementation does more work before the write, and thus
has faster writes.

We can thus see a tradeoff between these two implementations. If speed is a priority
in a write-heavy workload, then doing pre-allocation would be a good idea because
creating the file can reduce the write latency. However as mentioned previously,
pre-allocate potentially takes up more space as the entire file is allocated when
the full file may not be written to. Thus if storage takes priority, post-allocation
would be a better implementation as this can save us from allocating empty blocks
in a file and saves us from making requests to empty blocks 
(though in a typical file system this may not be common).

We can now compare these implementations to the batch allocation. As mentioned
in an earlier section, batch allocation works by batching the blocks sent to one
data node together, regardless of the file order. This saves on number of requests
sent by the metadata node.

We can see that the create latency is as fast as post-allocate because it only involves
metadata modification. However we can see a much lower write latency than both
the preallocate and postallocate implementations. This is because we batch blocks
to the same data nodes together. In the experiment, we have 100 blocks and 3 
datanodes. Using roundrobin, each block gets allocated in alternating round robin 
fashion. For preallocate and postallocate, there are 100 requests sent total
each of 1 block size. However in batch allocation, only 3 requests are sent, each
of size roughly 33 blocks. This is much faster because we write everything we
need to the datanode in a single request, removing the overhead of more requests
to the same datanode.

We can see that the read latency is also faster in the batch allocation because
similarly with the write, we make a single request for all of the involved blocks
per datanode. Thereby reducing the overhead in communication to the same datanode.

The total latency of the batch allocation is roughly 3x lower than both
implementations. We can thus see the benefits of batch allocation when sending
more than 1 block to each datanode. 


As mentioned in a previous experiment, modifying the block size can gives a benefit
because we write more data to each data node. However, as we make the block size
larger, there is less benefit to batching because there are less blocks to batch.
And as mentioned previously, the tradeoff of making blocks as large as possible
comes with wasting more storage as small files will use up only a fraction
of the total block.

% \experiment{
% 4 datanodes
% 16 MB file created, written to, and then read
% total size, 16 MB * 4 (so each node can fully hold file)
% averaged over 3 runs
% }

\begin{table}[H]
    \centering
    \caption{Batch Block Size Tuning}
    \label{t:batch_block}
    \begin{tabular}{l|l}
        Capacity & 64 MB \\
        Datanodes & 4 \\
        Policy & Round Robin \\
    \end{tabular}
\end{table}

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/block/read.png}}
\caption{}
\label{f:batch_block_read}
\end{figure}

Figure \ref{f:batch_block_read} shows the read time latency versus the block size for the above
experiment. We can see that the read latency drops as as block size increases
but somewhat stabilizes at larger block sizes.

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/block/write.png}}
\caption{}
\label{f:batch_block_write}
\end{figure}

Figure \ref{f:batch_block_write} shows the write latency versus the block size for the above experiment.
We can see that the write latency drops as block size increases but approaches
a minimum and stabilizes at larger block sizes.

Batching and block sizes are quite similar in distributed file systems. Batching
in this report refers to batching blocks together, sending the data to a single
data node. Extents in this report refers to increasing the block size past the
common 4096KB size. In terms of request size, batching and extents look the same.
Batching 2 blocks of 4096KB vs 1 extent of 8192KB looks roughly the same in the
simulator in terms of communication. The metadata node is sending roughly 8192KB
to a datanode to store (some bytes for the metadata difference since 2 blocks
require 2 integers identifying the block vs 1 integer to identify the extent).
However the difference is made in how the datanode handles the request. Batching
2 blocks reduces the number of requests sent from the metadata, however requires
the datanode to handle the creation of creating and writing two separate file
blocks in its own file system directory. In the case of the extent, the communication
overhead is the same as the batching, but it makes the datanode create and write
a single file with all of the data. Thus we can see that although batching brings
a benefit as it lowers the write and read latency, ultimately there is an overhead
of handling the different file blocks that increasing the extent size solves.
Having a larger extent looks the same as batching but has lower overhead in the
datanode. This is the reason why we observe the trends in Figures \ref{f:batch_block_read} and
\ref{f:batch_block_write}. As block size increases, the latency becomes lower since the datanode
has less blocks to handle, however as the block size grows to become similarly
sized to the file, the benefit of increasing the extents minimizes because
the latency is now driven by the writing to the file.

As mentioned before, increasing the extent size increases internal fragmentation
and wastes space. However too low of an extent size and the file system has
to handle more blocks which increases overhead in handling in the datanodes.
