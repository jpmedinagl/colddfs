\section{Results}
\label{s:results}

\subsection{Allocation Policy Effects}
\label{s:policy_effects}

The initial purpose of the simulator was to benchmark and investigate different
block allocation strategies on the simulator. Attempting to see the effects
of different workloads on the distributed file system.

However, due to time constraints, the simulator is entirely sequential. Meaning
requests are sent one at a time from the metadata node to the data nodes, and
requests are waited on by the metadata node until there's a response.

This means that different allocation policies did not seem to have a large
effect on the performance of the simulator.

\subsubsection{File Aware Tuning}
\label{s:fileawaretuning}
\

The file aware allocation policy works by switching between a random policy
versus least loaded when the number of allocated blocks at the same time are 
below a small file threshold.

These tests were ran on the implementation of preallocate. Meaning the file 
would be allocated all at once during creation, and thus we know whether it
will be a small file or large file depending on its size. On later implementations
like postallocate, this allocation policy will shift in meaning from small file
vs large file, to small allocation group vs large allocation group. This is 
because with lazy allocation, we might not allocate the entire file at once. 
This has the drawback that we could create a large file by doing small file 
allocations at a time. This will make it look like a small allocation group
every time and use random, and we would have a large file allocated distributed
randomly though it is a large file. For the later implementations, a more clever
file aware allocation policy should be created.

The small file threshold can be tuned since random allocation is much faster than
least loaded. In the following experiment, a block size of 4096KB was used

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/fileawaretune/avg_write_latency_ms4k.png}}
\caption{}
\label{f:}
\end{figure}

\subsection{File Distribution Impact}
\label{s:file_dist_impact}

\subsection{Data Node Scaling}
\label{s:datanode_scaling}

The number of data nodes in the application was expected to have an effect on
the performance of the application. However, due to the sequential nature of the
simulator, the data node number did not affect the performance of the file system.

Since the simulator was sequential, the metadata node sending a request to a 
datanode looks the same regardless of the data node. This is because nodes do
not have outstanding requests, each node handles one request at a time and 
responds to the metadata node. And thus sending to any data node looks the same.

Thus increasing the number of data nodes, wouldn't affect the application,
having a single data node handle all requests vs having 16 data nodes handle
the requests look the same in the simulator.

A future solution for this would be metadata node parallelism and datanode parallelism.
Though similar these two solutions are different. Metadata node parallelism means
that the metadata node handles each datanode in parallel, and thus can send requests
to two datanodes in parallel. However the metadata node cannot send two requests
to the same datanode since the datanodes are not parallel. With datanode parallelism,
this means that the datanodes can now handle multiple outstanding requests, meaning
the metadata node can send two requests to the same datanode and the datanode will
handle the requests in parallel.

Metadata node parallelism would not be difficult to implement, the metadata node
could create threads for each of the data nodes and each thread would wait until
the involved thread responds. Each thread would write (if its a read request)
into a different area of the buffer and thus would have no issues with corruption.

Datanode parallelism would be slightly more difficult to implement. This involves
locking mechanisms as now the datanode has to protect file blocks from being
read and written to at the same time.

With these new ideas, they could give an insight to data node scaling as the
metadata node can send more than one request to each data node. Increasing
the number of datanodes would likely increase performance, but at the cost
of metadata overhead and complexity.

\subsection{Optimal Block Size}
\label{s:extend_based_blocks}

The block size is a pretty important parameter in distributed file systems.
Distributed file systems do not have to adhere to any strict block sizes since
file contents are stored in data nodes own file system as files. This means
that blocks can be as small or large as required.

Small block sizes (4KB, 8KB) although used in file systems, are not ideal for
distributed file systems since distributed file systems communicate through the
network. This means that sending a larger request can be more beneficial as we're
already sending a request.

Larger block sizes can be beneficial, but too large of block sizes and the 
bottleneck of the simulator will be sending the requests.



\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/blocks/write_throughput_mbps.pdf}}
\caption{}
\label{f:}
\end{figure}

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/blocks/avg_write_latency_ms.pdf}}
\caption{}
\label{f:}
\end{figure}

Figure \ref{} outlines the experiment of measuring the average write latency versus
the block size. We can observe a minimum around the 32KB-64KB block size. This
means that for this workload, having block sizes wich are small/mid sized
allows us to minimize the write latency. This is because really small block sizes
increase the number of requests to the simulator, and this becomes the bottleneck.
Really large block sizes minimize the number of requests but lengthen the time
to process requests which becomes the bottleneck.  

\subsection{Implementation Benchmarks}
\label{s:implementation_bench}

In the previous section, it was outlined that various different implementations
were considered. Specifically, pre-allocate - allocating on file creation, 
post-allocate - allocating on write, batch-allocate - batch allocate blocks
sent to same node.

We can do a simple benchmark over these implementations and observe different
tradeoffs between them.

The experiment was done as follows.
% \experiment{
% 3 datanodes
% 100 block file created, written to, and then read
% total size file size * 3
% roundrobin
% block size 4096
% }

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/latency_impl.png}}
\caption{}
\label{f:}
\end{figure}

Figure \ref{} shows the results of the experiment. Each group of bars represents
a different implementation. Each bar in the group represents the different quantities
measured, the create latency, write latency, and read latency, as well as the total latency
shown in red.

We can observe that the total latency between the preallocate and postallocate 
implementations are the same. This is because in this case, there are no empty 
blocks in the file, all blocks are allocated during the write, and thus both 
implementations will take the same amount of total time. However we can see the 
difference between these two implementations by looking at the create and write 
latencies. Since pre-allocate allocates on create, the create latency is much larger
than post-allocate as post-allocate only modifies metadata. However we can see that
the write latency for the pre-allocate is much lower than post-allocate. This is
because the pre-allocate implementation does more work before the write, and thus
has faster writes.

We can thus see a tradeoff between these two implementations. If speed is a priority
in a write-heavy workload, then doing pre-allocation would be a good idea because
creating the file can reduce the write latency. However as mentioned previously,
pre-allocate potentially takes up more space as the entire file is allocated when
the full file may not be written to. Thus if storage takes priority, post-allocation
would be a better implementation as this can save us from allocating empty blocks
in a file and saves us from making requests to empty blocks 
(though in a typical file system this may not be common).

We can now compare these implementations to the batch allocation. As mentioned
in an earlier section, batch allocation works by batching the blocks sent to one
data node together, regardless of the file order. This saves on number of requests
sent by the metadata node.

We can see that the create latency is as fast as post-allocate because it only involves
metadata modification. However we can see a much lower write latency than both
the preallocate and postallocate implementations. This is because we batch blocks
to the same data nodes together. In the experiment, we have 100 blocks and 3 
datanodes. Using roundrobin, each block gets allocated in alternating round robin 
fashion. For preallocate and postallocate, there are 100 requests sent total
each of 1 block size. However in batch allocation, only 3 requests are sent, each
of size roughly 33 blocks. This is much faster because we write everything we
need to the datanode in a single request, removing the overhead of more requests
to the same datanode.

We can see that the read latency is also faster in the batch allocation because
similarly with the write, we make a single request for all of the involved blocks
per datanode. Thereby reducing the overhead in communication to the same datanode.

The total latency of the batch allocation is roughly 3x lower than both
implementations. We can thus see the benefits of batch allocation when sending
more than 1 block to each datanode. 


As mentioned in a previous experiment, modifying the block size can gives a benefit
because we write more data to each data node. However, as we make the block size
larger, there is less benefit to batching because there are less blocks to batch.
And as mentioned previously, the tradeoff of making blocks as large as possible
comes with wasting more storage as small files will use up only a fraction
of the total block.

In the following experiment, the block size is investigated against batching.
% \experiment{
% 4 datanodes
% 16 MB file created, written to, and then read
% total size, 16 MB * 4 (so each node can fully hold file)
% averaged over 3 runs
% }

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/block/read.png}}
\caption{}
\label{f:}
\end{figure}

Figure \ref{} shows the read time latency versus the block size for the above
experiment. We can see that the read latency drops as as block size increases
but somewhat stabilizes at larger block sizes.

\begin{figure}[H]
\centerline{\includegraphics[width=\columnwidth]{plots/batch/block/write.png}}
\caption{}
\label{f:}
\end{figure}

Figure \ref{} shows the write latency versus the block size for the above experiment.
We can see that the write latency drops as block size increases but approaches
a minimum and stabilizes at larger block sizes.

Batching and block sizes are quite similar in distributed file systems. Batching
in this report refers to batching blocks together, sending the data to a single
data node. Extents in this report refers to increasing the block size past the
common 4096KB size. In terms of request size, batching and extents look the same.
Batching 2 blocks of 4096KB vs 1 extent of 8192KB looks roughly the same in the
simulator in terms of communication. The metadata node is sending roughly 8192KB
to a datanode to store (some bytes for the metadata difference since 2 blocks
require 2 integers identifying the block vs 1 integer to identify the extent).
However the difference is made in how the datanode handles the request. Batching
2 blocks reduces the number of requests sent from the metadata, however requires
the datanode to handle the creation of creating and writing two separate file
blocks in its own file system directory. In the case of the extent, the communication
overhead is the same as the batching, but it makes the datanode create and write
a single file with all of the data. Thus we can see that although batching brings
a benefit as it lowers the write and read latency, ultimately there is an overhead
of handling the different file blocks that increasing the extent size solves.
Having a larger extent looks the same as batching but has lower overhead in the
datanode. This is the reason why we observe the trends in Figures \ref{} and
\ref{}. As block size increases, the latency becomes lower since the datanode
has less blocks to handle, however as the block size grows to become similarly
sized to the file, the benefit of increasing the extents minimizes because
the latency is now driven by the writing to the file.

As mentioned before, increasing the extent size increases internal fragmentation
and wastes space. However too low of an extent size and the file system has
to handle more blocks which increases overhead in handling in the datanodes.
